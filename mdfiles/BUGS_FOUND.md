# Complete Bug List - Found Through First Principles Testing

**Date**: 2025-10-02
**Method**: Manual verification from NASA data ‚Üí Services ‚Üí Dashboard
**Status**: All bugs identified, ready to fix

---

## üî¥ CRITICAL BUGS (Blocking Real Data Display)

### Bug #1: Wrong Method Name in Integration Service
**File**: `src/presentation/dashboard/services/dashboard_integration.py`
**Line**: 106
**Current Code**:
```python
data = self.data_loader.load_sensor_data(sensor_id)
```
**Error**: `AttributeError: 'NASADataLoader' object has no attribute 'load_sensor_data'`

**Fix**:
```python
data = self.data_loader.get_sensor_data(sensor_id)
```

**Impact**: Integration service falls back to mock data instead of loading real NASA data

---

### Bug #2: Wrong Method Name for Forecasting
**File**: `src/presentation/dashboard/services/dashboard_integration.py`
**Line**: 311
**Current Code**:
```python
forecast = self.forecasting_service.forecast(
    sensor_id=sensor_id,
    horizon=horizon,
    model_type=model_type
)
```
**Error**: `AttributeError: 'ForecastingService' object has no attribute 'forecast'`

**Fix**:
```python
forecast = self.forecasting_service.generate_forecast(
    sensor_id=sensor_id,
    data=historical['value'].values,
    timestamps=historical['timestamp'].tolist(),
    horizon_hours=horizon
)
```

**Impact**: Forecast method fails, falls back to simple statistical forecast

---

### Bug #3: Integration Service Not Imported in ANY Dashboard Tab
**Files**: All 15 dashboard layout files
**Current**: None of the layouts import or use `dashboard_integration.py`
**Fix**: Add to each layout:
```python
from src.presentation.dashboard.services.dashboard_integration import get_integration_service
```

**Affected Files**:
1. `anomaly_investigation.py` - ‚ùå Uses np.random for mock anomalies
2. `anomaly_monitor.py` - ‚ùå Uses mock data generators
3. `enhanced_forecasting.py` - ‚ùå Uses mock forecast data
4. `enhanced_maintenance_scheduler.py` - ‚ùå Uses mock schedule data
5. `mlflow_integration.py` - ‚ùå Uses mock experiment data
6. `monitoring.py` - ‚ùå Uses mock sensor readings
7. `overview.py` - ‚ùå Uses mock system status
8. `system_performance.py` - ‚ùå Uses mock metrics
9. `training_monitor.py` - ‚ùå Uses mock training jobs
10. `work_orders.py` - ‚ùå Uses mock work orders
11. `work_orders_simple.py` - ‚ùå Uses mock work orders

**Impact**: Dashboard displays only mock data, never connects to real NASA data or services

---

## üü° MEDIUM PRIORITY BUGS (API Mismatches)

### Bug #4: Incorrect AnomalyService API Usage
**File**: `src/presentation/dashboard/services/dashboard_integration.py`
**Lines**: 157-217
**Issue**: `detect_anomalies()` method doesn't call AnomalyService correctly

**Current Logic**:
```python
def detect_anomalies(self, sensor_id: str, data: np.ndarray) -> List[Dict]:
    # Calls SESSION 7 algorithms directly
    threshold_result = AdaptiveThresholdCalculator.consensus_threshold(data)
    prob_score = ProbabilisticAnomalyScorer.bayesian_anomaly_probability(...)
```

**Correct API**:
```python
def detect_anomalies(self, sensor_id: str, data: np.ndarray, timestamps: List[datetime]) -> List[Dict]:
    # Should call AnomalyService which already uses SESSION 7 algorithms
    result = self.anomaly_service.detect_anomalies(sensor_id, data, timestamps)
    return result['anomalies']
```

**Impact**: Bypasses AnomalyService, duplicates logic, doesn't use trained models

---

### Bug #5: Missing Timestamps in detect_anomalies
**File**: `src/presentation/dashboard/services/dashboard_integration.py`
**Line**: 157
**Current Signature**:
```python
def detect_anomalies(self, sensor_id: str, data: np.ndarray) -> List[Dict]:
```

**Issue**: AnomalyService requires timestamps, but integration method doesn't accept them

**Correct Signature**:
```python
def detect_anomalies(self, sensor_id: str, data: np.ndarray, timestamps: List[datetime]) -> List[Dict]:
```

**Impact**: Can't properly timestamp anomaly detections

---

### Bug #6: get_sensor_data Returns Wrong Format for Forecasting
**File**: `src/presentation/dashboard/services/dashboard_integration.py`
**Lines**: 305-326
**Issue**: `generate_forecast()` gets historical data via `get_sensor_data()` which returns DataFrame, but then tries to call ForecastingService which needs numpy array + timestamps

**Current**:
```python
historical = self.get_sensor_data(sensor_id, hours=168)  # Returns DataFrame
# Then tries to call:
forecast = self.forecasting_service.forecast(sensor_id=sensor_id, ...)  # Wrong!
```

**Should be**:
```python
historical = self.get_sensor_data(sensor_id, hours=168)  # DataFrame
forecast = self.forecasting_service.generate_forecast(
    sensor_id=sensor_id,
    data=historical['value'].values,  # numpy array
    timestamps=historical['timestamp'].tolist(),  # list of datetime
    horizon_hours=horizon
)
```

---

## üü¢ LOW PRIORITY (Cosmetic/Enhancement)

### Bug #7: Inconsistent Error Handling
**File**: Multiple dashboard layouts
**Issue**: Some callbacks have try/except, others don't
**Fix**: Standardize error handling across all callbacks
**Impact**: Inconsistent user experience when errors occur

---

### Bug #8: No User Feedback for Fallback Mode
**Issue**: When models aren't trained, services use fallback methods silently
**Current**: User sees results but doesn't know it's fallback mode
**Fix**: Add badge/indicator showing "Statistical Fallback" or "Model Training Required"
**Impact**: User confusion about data source

---

## üìä BUG SUMMARY STATISTICS

| Priority | Count | Status |
|----------|-------|--------|
| üî¥ Critical | 3 | Blocks real data display |
| üü° Medium | 4 | Causes API errors/fallbacks |
| üü¢ Low | 2 | Cosmetic/UX issues |
| **Total** | **9** | **All identified** |

---

## üîß FIX STRATEGY

### Phase 1: Fix Integration Service (30 min)
**Files to fix**: 1
- `dashboard_integration.py` (Bugs #1, #2, #4, #5, #6)

**Changes**:
1. Line 106: `load_sensor_data` ‚Üí `get_sensor_data`
2. Line 311: Update `forecast()` call to `generate_forecast()` with correct params
3. Lines 157-217: Rewrite `detect_anomalies()` to call AnomalyService
4. Add timestamps parameter to `detect_anomalies()`
5. Fix data format conversions in `generate_forecast()`

### Phase 2: Wire Dashboard Tabs (2-3 hours)
**Files to fix**: 11
- All layout files that use mock data (Bug #3)

**Pattern for each file**:
```python
# Add import at top
from src.presentation.dashboard.services.dashboard_integration import get_integration_service

# In callback, replace mock data with:
integration = get_integration_service()
real_data = integration.get_sensor_data(sensor_id, hours=24)
```

**Order of implementation**:
1. `monitoring.py` - Simplest, just sensor data
2. `overview.py` - System summary
3. `anomaly_monitor.py` - Anomaly detection
4. `enhanced_forecasting.py` - Forecasting
5. `anomaly_investigation.py` - Advanced anomaly analysis
6. Remaining tabs (maintenance, work orders, etc.)

### Phase 3: Polish (1 hour)
**Improvements**:
- Add error handling (Bug #7)
- Add fallback mode indicators (Bug #8)
- Test all tabs manually in browser

---

## ‚úÖ VERIFICATION CHECKLIST

After fixes, verify:

### Integration Service Tests:
- [ ] `integration.get_sensor_data()` returns real NASA data
- [ ] `integration.detect_anomalies()` calls AnomalyService correctly
- [ ] `integration.generate_forecast()` calls ForecastingService correctly
- [ ] No exceptions thrown for normal operations

### Dashboard Tab Tests (for each tab):
- [ ] Tab loads without errors
- [ ] Charts display data (not empty)
- [ ] Data matches NASA sensor data
- [ ] Timestamps are correct
- [ ] No browser console errors

### Manual Browser Testing:
- [ ] Visit http://127.0.0.1:8050
- [ ] Click each tab
- [ ] Verify real data displays
- [ ] Check for error messages
- [ ] Test interactive features

---

## üìù TESTING COMMANDS

### Test Integration Service After Fixes:
```bash
python3 << 'EOF'
from src.presentation.dashboard.services.dashboard_integration import get_integration_service

service = get_integration_service()

# Test 1: Sensor data
df = service.get_sensor_data('SMAP-PWR-001', hours=24)
print(f"Sensor data: {len(df)} rows, columns: {df.columns.tolist()}")

# Test 2: Anomaly detection
anomalies = service.detect_anomalies('SMAP-PWR-001', df['value'].values, df['timestamp'].tolist())
print(f"Anomalies: {len(anomalies)} found")

# Test 3: Forecasting
forecast = service.generate_forecast('SMAP-PWR-001', horizon=24)
print(f"Forecast: {len(forecast['forecast'])} points")

print("‚úì All tests passed!")
EOF
```

### Test Dashboard After Fixes:
```bash
# Start dashboard
python launch_complete_dashboard.py

# In browser, check:
# 1. http://127.0.0.1:8050 - Overview tab shows real data
# 2. Click Monitoring - Shows NASA sensor data
# 3. Click Anomalies - Shows real detections
# 4. Click Forecasting - Shows real predictions
```

---

## üéØ EXPECTED OUTCOME

After all fixes:
- ‚úÖ Dashboard displays real NASA SMAP/MSL data
- ‚úÖ Anomaly detection shows real algorithm results
- ‚úÖ Forecasting shows real predictions
- ‚úÖ All charts populated with actual data
- ‚úÖ No mock data generators in use
- ‚úÖ Services properly connected end-to-end

---

**Next Step**: Fix Bug #1 and #2 in integration service, then test before proceeding to wire dashboard tabs.
